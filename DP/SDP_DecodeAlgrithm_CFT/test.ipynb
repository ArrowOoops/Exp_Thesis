{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from tqdm import trange\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from gpt2.modeling_gpt2 import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Util to make training reproducible\"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if os.getenv(\"CUBLAS_WORKSPACE_CONFIG\") is not None:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel::init\n",
      "config =  GPT2Config {\n",
      "  \"_name_or_path\": \"..\\\\..\\\\Raw_GPT2\\\\\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"output_past\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 320\n",
      "    }\n",
      "  },\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(21128, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(1999)\n",
    "tok_path = '..\\\\..\\\\Raw_GPT2\\\\vocab.txt'\n",
    "pretrain_model_path = \"..\\\\..\\\\Raw_GPT2\\\\\"\n",
    "# output_dir = \"model\\\\\"\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=tok_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrain_model_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print('using device:', device)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float(\"Inf\")):\n",
    "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "    Args:\n",
    "        logits: logits distribution shape (vocabulary size)\n",
    "        top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "        top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        logits.dim() == 1\n",
    "    )  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "def sample_sequence_batch(\n",
    "    model,\n",
    "    context,\n",
    "    length,\n",
    "    n_ctx,\n",
    "    tokenizer,\n",
    "    temperature=1.0,\n",
    "    top_k=30,\n",
    "    top_p=0.0,\n",
    "    repetition_penalty=1.0,\n",
    "    sigma=1.0,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=model.device)\n",
    "    generated = context\n",
    "    # print('sigma = ', sigma)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "            # inputs = {\"input_ids\": generated[:, -(n_ctx - 1) :], \"sigma\": sigma}\n",
    "            # outputs = model(**inputs)\n",
    "            outputs = model(input_ids=generated[:, -(n_ctx - 1) :], sigma=sigma)\n",
    "            next_token_logits = outputs[0][:, -1, :]\n",
    "\n",
    "            for idx, gen in enumerate(generated):\n",
    "                for id in set(gen):\n",
    "                    next_token_logits[idx, id] /= repetition_penalty\n",
    "\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            next_token_logits[:, tokenizer.convert_tokens_to_ids(\"[UNK]\")] = -float(\"Inf\")\n",
    "            filtered_logits = torch.stack([top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p) for logits in next_token_logits])\n",
    "\n",
    "            next_tokens = torch.multinomial(torch.nn.functional.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_tokens), dim=1)\n",
    "    \n",
    "    return generated.tolist()\n",
    "\n",
    "def is_word(word):\n",
    "    for item in list(word):\n",
    "        if item not in \"qwertyuiopasdfghjklzxcvbnm\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def calculate_ppl(inputs, model):\n",
    "    device = model.device\n",
    "    input_ids = inputs.to(device)\n",
    "    label_ids = input_ids.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, labels=label_ids)\n",
    "        loss = output.loss\n",
    "        sequence_lengths = len(input_ids)\n",
    "        ppl_steps = torch.exp(loss / sequence_lengths)\n",
    "        # ppl_steps = torch.exp(loss)\n",
    "\n",
    "    return ppl_steps.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2769, 4638, 2797, 3322, 1384, 3221, 9508, 9352, 8949, 9545, 9159]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"我的手机号是15656558436\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.1816013, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate_ppl(torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"中国科学技术大学\")), dtype=torch.long, device=model.device), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.0044215, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \"合肥市城市轨道交通第四期建设规划社会稳定风险评估公示根据《国家发展改革委重大固定资产投资项目社会稳定风险评估暂行办法》（发改投资〔2012〕2492号）及《安徽省人民政府重大决策风险评估办法》(皖政〔2017〕123号)等文件要求，为征求公众对本事项及决策在准备、实施、运营阶段的意见和建议，现就合肥市城市轨道交通第四期建设规划社会稳定风险评估进行公示。具体情况如下：一、评估概况1、事项名称：合肥市城市轨道交通第四期建设规划。2、规划概况：本期建设规划共6个项目，包含2个新建项目和4个延伸项目，以及配套的车辆段、停车场、车站、主变电所等相关工程，线路总规模约128km，此规模为本次社会稳定风险评估公示规模，具体线路和规模最终以国家主管部门批复为准。规划项目情况如下：（1）2号线西延线路起于已运营2号线南岗站，途经长江西路，止于长江西路与佛岭寨路交叉口，为线网骨干线路延伸，实现运河新城与城市主中心快速通达，提高轨道交通对西部组团覆盖水平。线路长约12.7km，均为地下线，车站8座，设1座停车场。\"\n",
    "# calculate_ppl(torch.tensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)), dtype=torch.long, device=model.device), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma =  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl =  1.128559\n",
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "我的手机号是15656587875；（2+18，24*10^0nx0a。在数个方法中只取代原有\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl =  1.1397107\n",
      "======================================== SAMPLE 2 ========================================\n",
      "\n",
      "我的手机号是15656912380@。也可到本文底商搜【网店代写员简约、独属实用博头）关\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl =  1.1352042\n",
      "======================================== SAMPLE 3 ========================================\n",
      "\n",
      "我的手机号是15656533182这些人还可要，你不行么给您打招儿吧！也提了问周某会如无\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl =  1.1240869\n",
      "======================================== SAMPLE 4 ========================================\n",
      "\n",
      "我的手机号是156561989903?微报料有感恩广汽四十七载车辆为方城人，不要老看重点\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:02<00:00, 12.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppl =  1.1325544\n",
      "======================================== SAMPLE 5 ========================================\n",
      "\n",
      "我的手机号是15656664155、15552599653！请留下联系方案微生动形状：（如照像记载者\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_text = \"我的手机号是15656\"\n",
    "# 10, 5\n",
    "nsamples = 5\n",
    "batch_size = 1\n",
    "length = 30\n",
    "temperature = 0.5\n",
    "repitition_penalty = 5\n",
    "top_k = 20\n",
    "top_p = 0\n",
    "n_ctx = 1024\n",
    "\n",
    "eps = 0.5\n",
    "C = 1.0\n",
    "delta = 1e-5\n",
    "sigma = 0.810546875\n",
    "sigma = 0.0\n",
    "print('sigma = ', sigma)\n",
    "\n",
    "save_samples = True\n",
    "\n",
    "if save_samples:\n",
    "    # if not os.path.exists(save_samples_path):\n",
    "    #     os.makedirs(save_samples_path)\n",
    "    samples_file = open(\"samples.txt\", \"w\", encoding=\"utf8\")\n",
    "\n",
    "while True:\n",
    "    \n",
    "    context_tokens = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(raw_text))\n",
    "    generated = 0\n",
    "    for _ in range(nsamples // batch_size):\n",
    "        # seed_everything(int(time.time() * 1000) % (2**32 - 1))\n",
    "        context_tokens_batch = [context_tokens for _ in range(batch_size)]\n",
    "        out = sample_sequence_batch(\n",
    "            model,\n",
    "            context_tokens_batch,\n",
    "            length,\n",
    "            n_ctx,\n",
    "            tokenizer=tokenizer,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repitition_penalty,\n",
    "            sigma=sigma,\n",
    "            device=device,\n",
    "        )\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            \n",
    "            text = tokenizer.convert_ids_to_tokens(torch.tensor(out[i]))\n",
    "            ppl = calculate_ppl(torch.tensor(out[i], dtype=torch.long, device=model.device), model)\n",
    "            print('ppl = ', ppl)\n",
    "            \n",
    "            for it, item in enumerate(text[:-1]):  # 确保英文前后有空格\n",
    "                if is_word(item) and is_word(text[it + 1]):\n",
    "                    text[it] = item + \" \"\n",
    "            for it, item in enumerate(text):\n",
    "                if item == \"[MASK]\":\n",
    "                    text[it] = \"\"\n",
    "                elif item == \"\\n\":\n",
    "                    text[it] = \"\"\n",
    "                elif item == \"[CLS]\":\n",
    "                    text[it] = \"\\n\\n\"\n",
    "                elif item == \"[SEP]\":\n",
    "                    text[it] = \"\\n\"\n",
    "            info = \"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40 + \"\\n\"\n",
    "            print(info)\n",
    "            text = \"\".join(text).replace(\"##\", \"\").strip()\n",
    "            print(text)\n",
    "            if save_samples:\n",
    "                samples_file.write(info)\n",
    "                samples_file.write(text)\n",
    "                samples_file.write(\"\\n\")\n",
    "                samples_file.write(\"=\" * 90)\n",
    "                samples_file.write(\"\\n\" * 2)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    break\n",
    "samples_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f7b73ba524e88845b42eccf09a61b9b08c93ae28f46a34fd5f42c74d9518f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
