{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Util to make training reproducible\"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if os.getenv(\"CUBLAS_WORKSPACE_CONFIG\") is not None:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def worker_init(worked_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def set_logger(path):\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "    handler = logging.FileHandler(path + \"/train_log.txt\")\n",
    "    logger.setLevel(level=logging.INFO)\n",
    "    handler.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter(\n",
    "        \"%(asctime)s - %(filename)s - %(funcName)s - %(lineno)s - %(levelname)s\\n%(message)s\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    console = logging.StreamHandler()\n",
    "    console.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.addHandler(console)\n",
    "\n",
    "\n",
    "class Chinese_Medical_DS(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=1024):\n",
    "        self.path = path\n",
    "        sentence = []\n",
    "        with open(self.path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sen_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line.strip()))\n",
    "                full_sen = []\n",
    "                full_sen.append(tokenizer.convert_tokens_to_ids('[MASK]'))\n",
    "                full_sen.extend(sen_ids)\n",
    "                full_sen.append(tokenizer.convert_tokens_to_ids('[CLS]'))\n",
    "                if len(full_sen) <= max_len:\n",
    "                    sentence.append(full_sen)\n",
    "        self.data = sentence\n",
    "        \n",
    "    # need to overload\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # need to overload\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.data[idx]\n",
    "        target = input\n",
    "        return input, target\n",
    "    \n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, save_path, patience=2, verbose=True, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            save_path : 模型保存文件夹\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Saves model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n",
    "            )\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        model_to_save.save_pretrained(self.save_path + 'best_model')\n",
    "        \n",
    "        # path = os.path.join(self.save_path, \"best_network.pth\")\n",
    "        # torch.save(model.state_dict(), path)  # 这里会存储迄今最优模型的参数\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "tok_path = '..\\\\Raw_GPT2\\\\vocab.txt'\n",
    "pretrain_model_path = \"..\\\\Raw_GPT2\\\\\"\n",
    "output_dir = \"model\\\\\"\n",
    "\n",
    "epochs = 50\n",
    "warmup_steps = 1000\n",
    "lr = 1e-5\n",
    "gradient_accumulation = 18\n",
    "max_grad_norm = 1.0\n",
    "log_step = 10000\n",
    "set_logger(output_dir)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:06:33 - 945538542.py - <module> - 4 - INFO\n",
      "using device:cuda\n",
      "2023-03-27 17:06:33 - 945538542.py - <module> - 7 - INFO\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(21128, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer(vocab_file=tok_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(pretrain_model_path)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logger.info('using device:{}'.format(device))\n",
    "model.train()\n",
    "model.to(device)\n",
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\VSCode\\Experiment_Thesis\\CFT_GPT2\\train.ipynb 单元格 5\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m Chinese_Medical_DS(\u001b[39m\"\u001b[39;49m\u001b[39m..\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mData\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mtrain.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, tokenizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(dataset\u001b[39m=\u001b[39mtrain_dataset, worker_init_fn\u001b[39m=\u001b[39mworker_init)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m valid_dataset \u001b[39m=\u001b[39m Chinese_Medical_DS(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mData\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mvalid.txt\u001b[39m\u001b[39m\"\u001b[39m, tokenizer)\n",
      "\u001b[1;32md:\\VSCode\\Experiment_Thesis\\CFT_GPT2\\train.ipynb 单元格 5\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m sentence \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         sen_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids(tokenizer\u001b[39m.\u001b[39mtokenize(line\u001b[39m.\u001b[39mstrip()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/VSCode/Experiment_Thesis/CFT_GPT2/train.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         full_sen \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\dl\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = Chinese_Medical_DS(\"..\\\\Data\\\\train.txt\", tokenizer)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, worker_init_fn=worker_init)\n",
    "valid_dataset = Chinese_Medical_DS(\"..\\\\Data\\\\valid.txt\", tokenizer)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, worker_init_fn=worker_init)\n",
    "logger.info(\"len(train_dataloader), len(valid_dataloader) = {}, {}\".format(len(train_dataloader), len(valid_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                          num_training_steps=len(train_dataloader))\n",
    "tb_path = output_dir + \"/tb\"\n",
    "if not os.path.exists(tb_path):\n",
    "    os.mkdir(tb_path)\n",
    "writer = SummaryWriter(tb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 09:27:33 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 1\n",
      "2023-03-26 09:27:33 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 09:27:33.107631\n",
      "epoch-1:  11%|█         | 9998/90000 [09:23<1:15:33, 17.65it/s, loss=1.2352529]2023-03-26 09:36:56 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 9:36. Step 555 of epoch 1, loss 0.007813431648910046\n",
      "epoch-1:  22%|██▏       | 19998/90000 [18:38<1:09:17, 16.84it/s, loss=1.8927422]2023-03-26 09:46:11 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 9:46. Step 1111 of epoch 1, loss 0.006947780641168356\n",
      "epoch-1:  33%|███▎      | 29999/90000 [27:28<53:40, 18.63it/s, loss=1.6036086]  2023-03-26 09:55:02 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 9:55. Step 1666 of epoch 1, loss 0.006314156853407621\n",
      "epoch-1:  44%|████▍     | 39999/90000 [36:24<44:11, 18.86it/s, loss=2.2541289]  2023-03-26 10:03:57 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 10:3. Step 2222 of epoch 1, loss 0.006171812727302313\n",
      "epoch-1:  56%|█████▌    | 49999/90000 [45:11<34:16, 19.45it/s, loss=2.4748278]2023-03-26 10:12:44 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 10:12. Step 2777 of epoch 1, loss 0.006425004997104407\n",
      "epoch-1:  67%|██████▋   | 59999/90000 [53:53<26:42, 18.72it/s, loss=2.2535090]2023-03-26 10:21:26 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 10:21. Step 3333 of epoch 1, loss 0.006779309625178575\n",
      "epoch-1:  78%|███████▊  | 69998/90000 [1:02:39<16:37, 20.06it/s, loss=2.2949717]2023-03-26 10:30:13 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 10:30. Step 3888 of epoch 1, loss 0.0065022354405373335\n",
      "epoch-1:  89%|████████▉ | 79999/90000 [1:11:58<10:29, 15.88it/s, loss=1.2621639]2023-03-26 10:39:31 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 10:39. Step 4444 of epoch 1, loss 0.006315634371340275\n",
      "epoch-1: 100%|█████████▉| 89998/90000 [1:21:19<00:00, 18.88it/s, loss=3.4398732]2023-03-26 10:48:53 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 10:48. Step 5000 of epoch 1, loss 0.006176604108512402\n",
      "epoch-1: 100%|██████████| 90000/90000 [1:21:19<00:00, 18.44it/s, loss=1.2406316]\n",
      "2023-03-26 10:48:53 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 10:48:53 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 1\n",
      "2023-03-26 10:48:53 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 1 finished, train loss = 2.1313335896\n",
      "2023-03-26 10:48:53 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 10:48:53.691758\n",
      "2023-03-26 10:48:53 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:21:20.584127\n",
      "2023-03-26 10:48:53 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 1: 100%|██████████| 15000/15000 [05:22<00:00, 46.53it/s, loss=1.2200515]\n",
      "2023-03-26 10:54:16 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 10:54:16 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.9874094725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 1.987409).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 10:54:16 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 2\n",
      "2023-03-26 10:54:16 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 10:54:16.460477\n",
      "epoch-2:  11%|█         | 9999/90000 [09:14<1:13:13, 18.21it/s, loss=0.3034219]2023-03-26 11:03:30 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 11:3. Step 555 of epoch 2, loss 0.006110611184686423\n",
      "epoch-2:  22%|██▏       | 19998/90000 [18:22<1:07:57, 17.17it/s, loss=1.5156682]2023-03-26 11:12:39 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 11:12. Step 1111 of epoch 2, loss 0.005857782790530473\n",
      "epoch-2:  33%|███▎      | 29999/90000 [27:27<53:15, 18.78it/s, loss=1.3645127]  2023-03-26 11:21:44 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 11:21. Step 1666 of epoch 2, loss 0.005708108245022595\n",
      "epoch-2:  44%|████▍     | 39998/90000 [36:51<47:45, 17.45it/s, loss=2.0279479]  2023-03-26 11:31:07 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 11:31. Step 2222 of epoch 2, loss 0.005530866366997361\n",
      "epoch-2:  56%|█████▌    | 49998/90000 [45:56<35:31, 18.77it/s, loss=2.2143290]  2023-03-26 11:40:12 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 11:40. Step 2777 of epoch 2, loss 0.0059378289803862574\n",
      "epoch-2:  67%|██████▋   | 59998/90000 [54:52<27:34, 18.13it/s, loss=2.1498246]  2023-03-26 11:49:09 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 11:49. Step 3333 of epoch 2, loss 0.006349044545739889\n",
      "epoch-2:  78%|███████▊  | 69999/90000 [1:04:00<17:08, 19.45it/s, loss=2.1780274]2023-03-26 11:58:16 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 11:58. Step 3888 of epoch 2, loss 0.00607970536313951\n",
      "epoch-2:  89%|████████▉ | 79998/90000 [1:12:54<09:18, 17.89it/s, loss=1.1232185]2023-03-26 12:07:11 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 12:7. Step 4444 of epoch 2, loss 0.005912771650031209\n",
      "epoch-2: 100%|█████████▉| 89999/90000 [1:21:52<00:00, 20.12it/s, loss=3.2368562]2023-03-26 12:16:09 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 12:16. Step 5000 of epoch 2, loss 0.005871906151995063\n",
      "epoch-2: 100%|██████████| 90000/90000 [1:21:53<00:00, 18.32it/s, loss=1.0934035]\n",
      "2023-03-26 12:16:09 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 12:16:09 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 2\n",
      "2023-03-26 12:16:09 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 2 finished, train loss = 1.9135545492\n",
      "2023-03-26 12:16:09 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 12:16:09.910279\n",
      "2023-03-26 12:16:09 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:21:53.449802\n",
      "2023-03-26 12:16:09 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 2: 100%|██████████| 15000/15000 [05:06<00:00, 48.89it/s, loss=1.1992661]\n",
      "2023-03-26 12:21:16 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 12:21:16 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.8753641844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.987409 --> 1.875364).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 12:21:17 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 3\n",
      "2023-03-26 12:21:17 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 12:21:17.165731\n",
      "epoch-3:  11%|█         | 9999/90000 [08:59<1:13:49, 18.06it/s, loss=0.2567370]2023-03-26 12:30:16 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 12:30. Step 555 of epoch 3, loss 0.005662889221962541\n",
      "epoch-3:  22%|██▏       | 19998/90000 [17:55<1:06:23, 17.57it/s, loss=1.4556347]2023-03-26 12:39:12 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 12:39. Step 1111 of epoch 3, loss 0.0055732601010240615\n",
      "epoch-3:  33%|███▎      | 29997/90000 [26:47<52:36, 19.01it/s, loss=1.1073730]  2023-03-26 12:48:04 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 12:48. Step 1666 of epoch 3, loss 0.005502921638078987\n",
      "epoch-3:  44%|████▍     | 39997/90000 [35:43<47:17, 17.62it/s, loss=1.9999142]  2023-03-26 12:57:00 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 12:57. Step 2222 of epoch 3, loss 0.005262369329482317\n",
      "epoch-3:  56%|█████▌    | 49998/90000 [44:45<36:02, 18.49it/s, loss=2.1412206]  2023-03-26 13:06:02 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 13:6. Step 2777 of epoch 3, loss 0.005708880752697587\n",
      "epoch-3:  67%|██████▋   | 59998/90000 [53:47<27:20, 18.29it/s, loss=2.0932047]2023-03-26 13:15:04 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 13:15. Step 3333 of epoch 3, loss 0.006140869715809822\n",
      "epoch-3:  78%|███████▊  | 69999/90000 [1:02:49<18:10, 18.34it/s, loss=2.1236968]2023-03-26 13:24:07 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 13:24. Step 3888 of epoch 3, loss 0.005858914591185749\n",
      "epoch-3:  89%|████████▉ | 79999/90000 [1:12:02<09:39, 17.25it/s, loss=1.0973480]2023-03-26 13:33:19 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 13:33. Step 4444 of epoch 3, loss 0.005715350349992513\n",
      "epoch-3: 100%|█████████▉| 89998/90000 [1:21:10<00:00, 19.66it/s, loss=3.0924990]2023-03-26 13:42:27 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 13:42. Step 5000 of epoch 3, loss 0.005710442152805626\n",
      "epoch-3: 100%|██████████| 90000/90000 [1:21:10<00:00, 18.48it/s, loss=1.0279422]\n",
      "2023-03-26 13:42:27 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 13:42:27 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 3\n",
      "2023-03-26 13:42:27 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 3 finished, train loss = 1.8357217312\n",
      "2023-03-26 13:42:27 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 13:42:27.999001\n",
      "2023-03-26 13:42:27 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:21:10.833270\n",
      "2023-03-26 13:42:28 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 3: 100%|██████████| 15000/15000 [05:07<00:00, 48.85it/s, loss=1.1748543]\n",
      "2023-03-26 13:47:35 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 13:47:35 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.8146770000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.875364 --> 1.814677).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 13:47:35 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 4\n",
      "2023-03-26 13:47:35 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 13:47:35.491001\n",
      "epoch-4:  11%|█         | 9999/90000 [08:52<1:10:51, 18.82it/s, loss=0.1929289]2023-03-26 13:56:28 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 13:56. Step 555 of epoch 4, loss 0.005457634353544563\n",
      "epoch-4:  22%|██▏       | 19998/90000 [17:38<1:05:16, 17.88it/s, loss=1.4726558]2023-03-26 14:05:13 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 14:5. Step 1111 of epoch 4, loss 0.005402944914437831\n",
      "epoch-4:  33%|███▎      | 29999/90000 [26:18<51:50, 19.29it/s, loss=0.9365353]  2023-03-26 14:13:54 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 14:13. Step 1666 of epoch 4, loss 0.0053678151657804846\n",
      "epoch-4:  44%|████▍     | 39999/90000 [35:03<43:06, 19.33it/s, loss=1.9493117]  2023-03-26 14:22:39 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 14:22. Step 2222 of epoch 4, loss 0.005081958138756454\n",
      "epoch-4:  56%|█████▌    | 49998/90000 [43:41<33:20, 20.00it/s, loss=1.9773155]  2023-03-26 14:31:17 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 14:31. Step 2777 of epoch 4, loss 0.005561937124095857\n",
      "epoch-4:  67%|██████▋   | 59997/90000 [52:13<25:47, 19.39it/s, loss=2.0429382]2023-03-26 14:39:49 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 14:39. Step 3333 of epoch 4, loss 0.006014227935113013\n",
      "epoch-4:  78%|███████▊  | 69998/90000 [1:01:04<16:42, 19.95it/s, loss=2.0976200]2023-03-26 14:48:40 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 14:48. Step 3888 of epoch 4, loss 0.005720772910304367\n",
      "epoch-4:  89%|████████▉ | 79999/90000 [1:09:56<09:21, 17.80it/s, loss=1.0284992]2023-03-26 14:57:32 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 14:57. Step 4444 of epoch 4, loss 0.005575813757814467\n",
      "epoch-4: 100%|█████████▉| 89999/90000 [1:18:54<00:00, 20.29it/s, loss=3.0778992]2023-03-26 15:06:30 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 15:6. Step 5000 of epoch 4, loss 0.005587422675266862\n",
      "epoch-4: 100%|██████████| 90000/90000 [1:18:54<00:00, 19.01it/s, loss=1.0397034]\n",
      "2023-03-26 15:06:30 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 15:06:30 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 4\n",
      "2023-03-26 15:06:30 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 4 finished, train loss = 1.7877087593\n",
      "2023-03-26 15:06:30 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 15:06:30.630906\n",
      "2023-03-26 15:06:30 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:18:55.139905\n",
      "2023-03-26 15:06:30 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 4: 100%|██████████| 15000/15000 [05:05<00:00, 49.05it/s, loss=1.1614050]\n",
      "2023-03-26 15:11:36 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 15:11:36 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.7761695385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.814677 --> 1.776170).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 15:11:36 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 5\n",
      "2023-03-26 15:11:36 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 15:11:36.828352\n",
      "epoch-5:  11%|█         | 9999/90000 [08:58<1:10:00, 19.05it/s, loss=0.2127250]2023-03-26 15:20:35 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 15:20. Step 555 of epoch 5, loss 0.005325427847821265\n",
      "epoch-5:  22%|██▏       | 19998/90000 [17:54<1:04:23, 18.12it/s, loss=1.4037279]2023-03-26 15:29:31 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 15:29. Step 1111 of epoch 5, loss 0.005286604707688093\n",
      "epoch-5:  33%|███▎      | 29998/90000 [26:45<52:39, 18.99it/s, loss=0.8346838]  2023-03-26 15:38:22 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 15:38. Step 1666 of epoch 5, loss 0.0052620055502280595\n",
      "epoch-5:  44%|████▍     | 39999/90000 [35:41<46:29, 17.92it/s, loss=1.9218628]  2023-03-26 15:47:18 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 15:47. Step 2222 of epoch 5, loss 0.004953988527692854\n",
      "epoch-5:  56%|█████▌    | 49998/90000 [44:29<34:54, 19.10it/s, loss=1.9264871]2023-03-26 15:56:06 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 15:56. Step 2777 of epoch 5, loss 0.005459111696295441\n",
      "epoch-5:  67%|██████▋   | 59999/90000 [53:10<27:26, 18.22it/s, loss=2.0276251]2023-03-26 16:04:47 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 16:4. Step 3333 of epoch 5, loss 0.005905038936249911\n",
      "epoch-5:  78%|███████▊  | 69999/90000 [1:01:57<17:13, 19.35it/s, loss=2.0174837]2023-03-26 16:13:34 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 16:13. Step 3888 of epoch 5, loss 0.005608930751122535\n",
      "epoch-5:  89%|████████▉ | 79998/90000 [1:10:48<09:15, 18.02it/s, loss=1.0299823]2023-03-26 16:22:25 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 16:22. Step 4444 of epoch 5, loss 0.00546721538938582\n",
      "epoch-5: 100%|█████████▉| 89998/90000 [1:19:45<00:00, 19.74it/s, loss=2.9525182]2023-03-26 16:31:22 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 16:31. Step 5000 of epoch 5, loss 0.005505237360112369\n",
      "epoch-5: 100%|██████████| 90000/90000 [1:19:46<00:00, 18.80it/s, loss=0.9997158]\n",
      "2023-03-26 16:31:22 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 16:31:22 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 5\n",
      "2023-03-26 16:31:23 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 5 finished, train loss = 1.7520437241\n",
      "2023-03-26 16:31:23 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 16:31:23.265109\n",
      "2023-03-26 16:31:23 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:19:46.436757\n",
      "2023-03-26 16:31:23 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 5: 100%|██████████| 15000/15000 [05:06<00:00, 48.91it/s, loss=1.1470783]\n",
      "2023-03-26 16:36:29 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 16:36:29 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.7482055426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.776170 --> 1.748206).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 16:36:30 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 6\n",
      "2023-03-26 16:36:30 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 16:36:30.360279\n",
      "epoch-6:  11%|█         | 9999/90000 [08:59<1:08:51, 19.36it/s, loss=0.1722431]2023-03-26 16:45:30 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 16:45. Step 555 of epoch 6, loss 0.005232632712740451\n",
      "epoch-6:  22%|██▏       | 19998/90000 [17:58<1:05:58, 17.68it/s, loss=1.4022651]2023-03-26 16:54:28 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 16:54. Step 1111 of epoch 6, loss 0.0051908160829916596\n",
      "epoch-6:  33%|███▎      | 29997/90000 [26:50<52:52, 18.91it/s, loss=0.7222533]  2023-03-26 17:03:21 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 17:3. Step 1666 of epoch 6, loss 0.005182975849322975\n",
      "epoch-6:  44%|████▍     | 39998/90000 [35:45<47:52, 17.41it/s, loss=1.9361275]  2023-03-26 17:12:15 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 17:12. Step 2222 of epoch 6, loss 0.004857331681251526\n",
      "epoch-6:  56%|█████▌    | 49999/90000 [44:32<33:06, 20.14it/s, loss=1.9136372]2023-03-26 17:21:03 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 17:21. Step 2777 of epoch 6, loss 0.005364925981964916\n",
      "epoch-6:  67%|██████▋   | 59997/90000 [53:14<26:52, 18.60it/s, loss=1.9601516]2023-03-26 17:29:45 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 17:29. Step 3333 of epoch 6, loss 0.00583233792334795\n",
      "epoch-6:  78%|███████▊  | 69997/90000 [1:02:00<16:13, 20.54it/s, loss=2.0636067]2023-03-26 17:38:31 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 17:38. Step 3888 of epoch 6, loss 0.0055293320111930375\n",
      "epoch-6:  89%|████████▉ | 79998/90000 [1:10:52<09:37, 17.32it/s, loss=1.0541483]2023-03-26 17:47:23 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 17:47. Step 4444 of epoch 6, loss 0.005381844882667065\n",
      "epoch-6: 100%|█████████▉| 89999/90000 [1:19:50<00:00, 19.58it/s, loss=2.9381404]2023-03-26 17:56:21 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 17:56. Step 5000 of epoch 6, loss 0.005437736179120838\n",
      "epoch-6: 100%|██████████| 90000/90000 [1:19:50<00:00, 18.79it/s, loss=1.0009037]\n",
      "2023-03-26 17:56:21 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 17:56:21 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 6\n",
      "2023-03-26 17:56:21 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 6 finished, train loss = 1.7244117260\n",
      "2023-03-26 17:56:21 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 17:56:21.455798\n",
      "2023-03-26 17:56:21 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:19:51.095519\n",
      "2023-03-26 17:56:21 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 6: 100%|██████████| 15000/15000 [05:07<00:00, 48.84it/s, loss=1.1363865]\n",
      "2023-03-26 18:01:28 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 18:01:28 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.7278143167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.748206 --> 1.727814).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 18:01:28 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 7\n",
      "2023-03-26 18:01:28 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 18:01:28.975441\n",
      "epoch-7:  11%|█         | 9999/90000 [09:00<1:11:06, 18.75it/s, loss=0.1780500]2023-03-26 18:10:29 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 18:10. Step 555 of epoch 7, loss 0.0051687027758918706\n",
      "epoch-7:  22%|██▏       | 19998/90000 [17:57<1:07:44, 17.22it/s, loss=1.4370263]2023-03-26 18:19:26 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 18:19. Step 1111 of epoch 7, loss 0.005121685974579304\n",
      "epoch-7:  33%|███▎      | 29999/90000 [26:49<52:27, 19.06it/s, loss=0.5652030]  2023-03-26 18:28:18 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 18:28. Step 1666 of epoch 7, loss 0.005115795264020563\n",
      "epoch-7:  44%|████▍     | 39999/90000 [35:44<43:11, 19.30it/s, loss=1.9016135]  2023-03-26 18:37:13 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 18:37. Step 2222 of epoch 7, loss 0.004778570753429085\n",
      "epoch-7:  56%|█████▌    | 49999/90000 [44:33<35:01, 19.03it/s, loss=1.8937821]2023-03-26 18:46:02 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 18:46. Step 2777 of epoch 7, loss 0.005293882070388645\n",
      "epoch-7:  67%|██████▋   | 59999/90000 [53:15<26:25, 18.92it/s, loss=1.9719700]2023-03-26 18:54:44 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 18:54. Step 3333 of epoch 7, loss 0.00576298153642565\n",
      "epoch-7:  78%|███████▊  | 69999/90000 [1:02:02<17:12, 19.37it/s, loss=2.0088191]2023-03-26 19:03:31 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 19:3. Step 3888 of epoch 7, loss 0.005455026603490115\n",
      "epoch-7:  89%|████████▉ | 79998/90000 [1:10:53<09:14, 18.03it/s, loss=1.0043238]2023-03-26 19:12:22 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 19:12. Step 4444 of epoch 7, loss 0.005305978032853455\n",
      "epoch-7: 100%|█████████▉| 89998/90000 [1:19:50<00:00, 20.21it/s, loss=2.9426534]2023-03-26 19:21:19 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 19:21. Step 5000 of epoch 7, loss 0.005373197199963033\n",
      "epoch-7: 100%|██████████| 90000/90000 [1:19:50<00:00, 18.79it/s, loss=1.0310218]\n",
      "2023-03-26 19:21:19 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 19:21:19 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 7\n",
      "2023-03-26 19:21:19 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 7 finished, train loss = 1.7017608881\n",
      "2023-03-26 19:21:19 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 19:21:19.977276\n",
      "2023-03-26 19:21:19 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:19:51.001835\n",
      "2023-03-26 19:21:19 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 7: 100%|██████████| 15000/15000 [05:06<00:00, 48.86it/s, loss=1.1207081]\n",
      "2023-03-26 19:26:26 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 19:26:26 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.7118870020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.727814 --> 1.711887).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 19:26:27 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 8\n",
      "2023-03-26 19:26:27 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 19:26:27.370592\n",
      "epoch-8:  11%|█         | 9999/90000 [08:59<1:10:24, 18.94it/s, loss=0.1960545]2023-03-26 19:35:26 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 19:35. Step 555 of epoch 8, loss 0.005106425412930548\n",
      "epoch-8:  22%|██▏       | 19998/90000 [17:54<1:05:26, 17.83it/s, loss=1.3948320]2023-03-26 19:44:21 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 19:44. Step 1111 of epoch 8, loss 0.0050581348762847485\n",
      "epoch-8:  33%|███▎      | 29999/90000 [26:45<51:57, 19.24it/s, loss=0.5386490]  2023-03-26 19:53:12 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 19:53. Step 1666 of epoch 8, loss 0.005054469848051667\n",
      "epoch-8:  44%|████▍     | 39999/90000 [35:40<44:50, 18.58it/s, loss=1.9366363]  2023-03-26 20:02:07 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 20:2. Step 2222 of epoch 8, loss 0.004713246468268335\n",
      "epoch-8:  56%|█████▌    | 49998/90000 [44:28<35:34, 18.74it/s, loss=1.8194242]  2023-03-26 20:10:55 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 20:10. Step 2777 of epoch 8, loss 0.0052360018466599285\n",
      "epoch-8:  67%|██████▋   | 59997/90000 [53:10<25:35, 19.54it/s, loss=1.9210846]2023-03-26 20:19:37 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 20:19. Step 3333 of epoch 8, loss 0.00571677385289222\n",
      "epoch-8:  78%|███████▊  | 69999/90000 [1:01:59<17:07, 19.47it/s, loss=1.9448001]2023-03-26 20:28:26 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 20:28. Step 3888 of epoch 8, loss 0.005390973044000566\n",
      "epoch-8:  89%|████████▉ | 79999/90000 [1:10:51<09:06, 18.31it/s, loss=0.9958082]2023-03-26 20:37:19 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 20:37. Step 4444 of epoch 8, loss 0.005256344678904862\n",
      "epoch-8: 100%|█████████▉| 89997/90000 [1:19:49<00:00, 19.06it/s, loss=2.9734640]2023-03-26 20:46:16 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 20:46. Step 5000 of epoch 8, loss 0.005325964050553739\n",
      "epoch-8: 100%|██████████| 90000/90000 [1:19:49<00:00, 18.79it/s, loss=0.9873291]\n",
      "2023-03-26 20:46:16 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 20:46:16 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 8\n",
      "2023-03-26 20:46:17 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 8 finished, train loss = 1.6830973625\n",
      "2023-03-26 20:46:17 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 20:46:17.337725\n",
      "2023-03-26 20:46:17 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:19:49.967133\n",
      "2023-03-26 20:46:17 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 8: 100%|██████████| 15000/15000 [05:07<00:00, 48.84it/s, loss=1.1089274]\n",
      "2023-03-26 20:51:24 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 20:51:24 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.6980623007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.711887 --> 1.698062).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 20:51:24 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 9\n",
      "2023-03-26 20:51:24 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 20:51:24.906402\n",
      "epoch-9:  11%|█         | 9998/90000 [08:58<1:10:05, 19.02it/s, loss=0.1822250]2023-03-26 21:00:23 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 21:0. Step 555 of epoch 9, loss 0.005061278734076768\n",
      "epoch-9:  22%|██▏       | 19999/90000 [17:54<1:12:26, 16.10it/s, loss=1.3826845]2023-03-26 21:09:19 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 21:9. Step 1111 of epoch 9, loss 0.005001440432015806\n",
      "epoch-9:  33%|███▎      | 29997/90000 [26:45<53:18, 18.76it/s, loss=0.5471426]  2023-03-26 21:18:10 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 21:18. Step 1666 of epoch 9, loss 0.005000369258970022\n",
      "epoch-9:  44%|████▍     | 39999/90000 [35:40<43:44, 19.05it/s, loss=1.8413776]  2023-03-26 21:27:05 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 21:27. Step 2222 of epoch 9, loss 0.0046660763918422166\n",
      "epoch-9:  56%|█████▌    | 49998/90000 [44:29<34:49, 19.15it/s, loss=1.8381611]  2023-03-26 21:35:54 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 21:35. Step 2777 of epoch 9, loss 0.005189106837846339\n",
      "epoch-9:  67%|██████▋   | 59999/90000 [53:39<27:13, 18.37it/s, loss=1.9842179]  2023-03-26 21:45:04 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 21:45. Step 3333 of epoch 9, loss 0.005653165712300688\n",
      "epoch-9:  78%|███████▊  | 69997/90000 [1:02:41<16:30, 20.19it/s, loss=1.9652014]2023-03-26 21:54:06 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 21:54. Step 3888 of epoch 9, loss 0.005343060090765357\n",
      "epoch-9:  89%|████████▉ | 79999/90000 [1:11:58<09:57, 16.73it/s, loss=0.9558265]2023-03-26 22:03:23 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 22:3. Step 4444 of epoch 9, loss 0.005210643252264708\n",
      "epoch-9: 100%|█████████▉| 89997/90000 [1:21:15<00:00, 20.47it/s, loss=2.9515891]2023-03-26 22:12:40 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 22:12. Step 5000 of epoch 9, loss 0.005283054262585938\n",
      "epoch-9: 100%|██████████| 90000/90000 [1:21:15<00:00, 18.46it/s, loss=0.9891405]\n",
      "2023-03-26 22:12:40 - 833870943.py - <module> - 58 - INFO\n",
      "train step = 89999\n",
      "2023-03-26 22:12:40 - 833870943.py - <module> - 62 - INFO\n",
      "saving model for epoch 9\n",
      "2023-03-26 22:12:40 - 833870943.py - <module> - 68 - INFO\n",
      "epoch 9 finished, train loss = 1.6670383215\n",
      "2023-03-26 22:12:40 - 833870943.py - <module> - 71 - INFO\n",
      "time: 2023-03-26 22:12:40.639233\n",
      "2023-03-26 22:12:40 - 833870943.py - <module> - 72 - INFO\n",
      "time for one epoch: 1:21:15.732831\n",
      "2023-03-26 22:12:40 - 833870943.py - <module> - 74 - INFO\n",
      "start validate\n",
      "valid 9: 100%|██████████| 15000/15000 [04:56<00:00, 50.55it/s, loss=1.1041147]\n",
      "2023-03-26 22:17:37 - 833870943.py - <module> - 90 - INFO\n",
      "valid step = 14999\n",
      "2023-03-26 22:17:37 - 833870943.py - <module> - 93 - INFO\n",
      "valid finished, valid loss = 1.6875236034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (1.698062 --> 1.687524).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 22:17:37 - 833870943.py - <module> - 6 - INFO\n",
      "epoch 10\n",
      "2023-03-26 22:17:37 - 833870943.py - <module> - 8 - INFO\n",
      "time: 2023-03-26 22:17:37.765235\n",
      "epoch-10:  11%|█         | 9999/90000 [08:49<1:12:05, 18.49it/s, loss=0.1530405]2023-03-26 22:26:26 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 22:26. Step 555 of epoch 10, loss 0.005020985192619264\n",
      "epoch-10:  22%|██▏       | 19999/90000 [17:51<1:07:32, 17.27it/s, loss=1.3855948]2023-03-26 22:35:29 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 22:35. Step 1111 of epoch 10, loss 0.004964002070203423\n",
      "epoch-10:  33%|███▎      | 29999/90000 [26:39<52:19, 19.11it/s, loss=0.4580060]  2023-03-26 22:44:17 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 22:44. Step 1666 of epoch 10, loss 0.004969878990389406\n",
      "epoch-10:  44%|████▍     | 39999/90000 [35:29<44:06, 18.90it/s, loss=1.8191639]  2023-03-26 22:53:07 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 22:53. Step 2222 of epoch 10, loss 0.004614406159799546\n",
      "epoch-10:  56%|█████▌    | 49999/90000 [44:39<40:32, 16.45it/s, loss=1.7594434]  2023-03-26 23:02:17 - 833870943.py - <module> - 48 - INFO\n",
      "now time: 23:2. Step 2777 of epoch 10, loss 0.00514850364010781\n",
      "epoch-10:  61%|██████    | 54456/90000 [49:56<2:39:23,  3.72it/s, loss=2.1463463]"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "early_stopping = EarlyStopping(output_dir)\n",
    "train_step_per_epoch = len(train_dataloader)\n",
    "valid_step_per_epoch = len(valid_dataloader)\n",
    "for epoch in range(epochs):\n",
    "    logger.info('epoch {}'.format(epoch + 1))\n",
    "    now = datetime.now()\n",
    "    logger.info('time: {}'.format(now))\n",
    "    model.train()\n",
    "    train_pbar = tqdm(train_dataloader)\n",
    "    all_train_loss = 0.0\n",
    "    train_pbar.set_description('epoch-' + str(epoch + 1))\n",
    "    for step, (input, label) in enumerate(train_pbar):\n",
    "        input_ids = torch.tensor(label).long().to(device)\n",
    "        label_ids = torch.tensor(input).long().to(device)\n",
    "\n",
    "        #  forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=label_ids)\n",
    "        loss, logits = outputs[:2]\n",
    "        \n",
    "        if gradient_accumulation > 1:\n",
    "            loss = loss / gradient_accumulation\n",
    "            \n",
    "        #  loss backward\n",
    "        # if fp16:\n",
    "        #     with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        #         scaled_loss.backward()\n",
    "        #         torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n",
    "        # else:\n",
    "        #     loss.backward()\n",
    "        #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        loss.backward()\n",
    "        loss = loss.detach()\n",
    "        all_train_loss += loss\n",
    "        \n",
    "        writer.add_scalar('loss/train_step_loss', scalar_value=loss * gradient_accumulation, global_step=epoch * train_step_per_epoch+step)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        #  optimizer step\n",
    "        if (step + 1) % gradient_accumulation == 0:\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "        if (step + 1) % log_step == 0:\n",
    "            logger.info('now time: {}:{}. Step {} of epoch {}, loss {}'.format(\n",
    "                datetime.now().hour,\n",
    "                datetime.now().minute,\n",
    "                (step + 1) // gradient_accumulation,\n",
    "                epoch + 1,\n",
    "                running_loss / log_step))\n",
    "            running_loss = 0\n",
    "        \n",
    "        train_pbar.set_postfix({'loss': '{:.7f}'.format(loss*gradient_accumulation)})\n",
    "        \n",
    "    logger.info('train step = {}'.format(step))\n",
    "    all_train_loss = all_train_loss / (step + 1)\n",
    "\n",
    "    writer.add_scalar('loss/train_epoch_loss', scalar_value=all_train_loss * gradient_accumulation, global_step=epoch + 1)\n",
    "    logger.info('saving model for epoch {}'.format(epoch + 1))\n",
    "    if not os.path.exists(output_dir + 'model_epoch{}'.format(epoch + 1)):\n",
    "        os.mkdir(output_dir + 'model_epoch{}'.format(epoch + 1))\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    model_to_save.save_pretrained(output_dir + 'model_epoch{}'.format(epoch + 1))\n",
    "\n",
    "    logger.info('epoch {} finished, train loss = {:.10f}'.format(epoch + 1, all_train_loss * gradient_accumulation))\n",
    "\n",
    "    then = datetime.now()\n",
    "    logger.info('time: {}'.format(then))\n",
    "    logger.info('time for one epoch: {}'.format(then - now))\n",
    "    \n",
    "    logger.info('start validate')\n",
    "    model.eval()\n",
    "    all_valid_loss = 0.0\n",
    "    valid_pbar = tqdm(valid_dataloader)\n",
    "    valid_pbar.set_description('valid ' + str(epoch + 1))\n",
    "    for step, (input, label) in enumerate(valid_pbar):\n",
    "        input_ids = torch.tensor(label).long().to(device)\n",
    "        label_ids = torch.tensor(input).long().to(device)\n",
    "\n",
    "        #  forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=label_ids)\n",
    "        loss = outputs[0].detach()\n",
    "        writer.add_scalar('loss/valid_step_loss', scalar_value=loss, global_step=epoch * valid_step_per_epoch + step)\n",
    "        all_valid_loss += loss\n",
    "        valid_pbar.set_postfix({'loss': '{:.7f}'.format(loss)})\n",
    "    \n",
    "    logger.info('valid step = {}'.format(step))\n",
    "    all_valid_loss = all_valid_loss / (step + 1)\n",
    "    writer.add_scalar('loss/valid_epoch_loss', scalar_value=all_valid_loss, global_step=epoch+1)\n",
    "    logger.info('valid finished, valid loss = {:.10f}'.format(all_valid_loss))\n",
    "    early_stopping(all_valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        logger.info(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "writer.close()    \n",
    "\n",
    "logger.info('training finished')\n",
    "if not os.path.exists(output_dir + 'final_model'):\n",
    "    os.mkdir(output_dir + 'final_model')\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir + 'final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f7b73ba524e88845b42eccf09a61b9b08c93ae28f46a34fd5f42c74d9518f42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
