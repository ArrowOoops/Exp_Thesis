{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir: Chinese_Medical_Dialogue_Data\n",
      "sub_dirs: []\n",
      "files: ['ErKe-14000.xlsx', 'FuChanKe-28000.xlsx', 'NanKe-13000.xlsx', 'NeiKe-33000.xlsx', 'WaiKe-14000.xlsx', 'ZhongLiuKe-10000.xlsx']\n"
     ]
    }
   ],
   "source": [
    "file_dir = 'Chinese_Medical_Dialogue_Data'  #你的文件路径\n",
    "def getFlist(path):\n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        print('root_dir:', root)  #当前路径\n",
    "        print('sub_dirs:', dirs)   #子文件夹\n",
    "        print('files:', files)     #文件名称，返回list类型\n",
    "    return files\n",
    "file_name = getFlist(file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chinese_Medical_Dialogue_Data\\\\ErKe-14000.xlsx',\n",
       " 'Chinese_Medical_Dialogue_Data\\\\FuChanKe-28000.xlsx',\n",
       " 'Chinese_Medical_Dialogue_Data\\\\NanKe-13000.xlsx',\n",
       " 'Chinese_Medical_Dialogue_Data\\\\NeiKe-33000.xlsx',\n",
       " 'Chinese_Medical_Dialogue_Data\\\\WaiKe-14000.xlsx',\n",
       " 'Chinese_Medical_Dialogue_Data\\\\ZhongLiuKe-10000.xlsx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = [file_dir + '\\\\' + item for item in file_name]\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:06<00:00, 11.05s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_all = []\n",
    "for name in tqdm(file_path):\n",
    "    df = pd.read_excel(name)\n",
    "    df['file_name'] = name\n",
    "    df_all.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:08<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "def regulate_expression(x):\n",
    "    x = x.replace('\\n', '')\n",
    "    if x[-1] == '吗' or x[-2:-1] == '原因' or (x[-1] != '？' and x[-1] != '?'):\n",
    "        x = x + '？'\n",
    "    return x\n",
    "\n",
    "\n",
    "def replace_line(x):\n",
    "    x = x.replace('\\n', '')\n",
    "    return x\n",
    "\n",
    "\n",
    "ndf_all = []\n",
    "\n",
    "\n",
    "for df in tqdm(df_all):\n",
    "    # 过滤ask为‘无’\n",
    "    find_index = df[df['ask'] == '无'].index.tolist()\n",
    "    tdf = df.drop(find_index).dropna()\n",
    "    tdf = tdf.reset_index(drop=True)\n",
    "    \n",
    "    # 过滤ask为非字符类型\n",
    "    idx = []\n",
    "    for i in range(len(tdf)):\n",
    "        if type(tdf['ask'][i]) != type(tdf['ask'][0]):\n",
    "            idx.append(i)\n",
    "            # print(tdf['ask'][i])\n",
    "    tdf.drop(tdf.index[idx], inplace=True)\n",
    "    tdf = tdf.reset_index(drop=True)\n",
    "    \n",
    "    # 生成完整输入句子\n",
    "    tdf['title'] = tdf['title'].apply(regulate_expression)\n",
    "    tdf['ask'] = tdf['ask'].apply(replace_line)\n",
    "    tdf['answer'] = tdf['answer'].apply(replace_line)\n",
    "    tdf['sequence'] = tdf['title'] + tdf['ask'] + tdf['answer']\n",
    "    \n",
    "    # 过滤长度大于320的句子\n",
    "    find_over_long_index = tdf.loc[tdf['sequence'].str.len() > 320].index.tolist()\n",
    "    len(find_over_long_index) / len(tdf)\n",
    "    tdf = tdf.drop(find_over_long_index).dropna().reset_index(drop=True)\n",
    "    tdf.reset_index(drop=True)\n",
    "    \n",
    "    ndf_all.append(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80241\n",
      "138586\n",
      "83140\n",
      "168853\n",
      "92569\n",
      "62542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62542"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_num = 1e10\n",
    "for df in tqdm(ndf_all):\n",
    "    min_num = min(len(df), min_num)\n",
    "    print(len(df))\n",
    "min_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 56.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62542\n",
      "62542\n",
      "62542\n",
      "62542\n",
      "62542\n",
      "62542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_all = []\n",
    "for df in tqdm(ndf_all):\n",
    "    sdf = df.sample(n=min_num)\n",
    "    print(len(sdf))\n",
    "    sdf_all.append(sdf.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_path = \"Preprocess_Data\\\\\"\n",
    "idx = 0\n",
    "for df in tqdm(sdf_all):\n",
    "    f_name = file_name[idx].split('-')[0]\n",
    "    df.to_csv(processed_path + f_name + '.csv')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 299.99it/s]\n"
     ]
    }
   ],
   "source": [
    "train_num = 15000\n",
    "valid_num = 2500\n",
    "test_num = 2500\n",
    "\n",
    "train_df_all = []\n",
    "valid_df_all = []\n",
    "test_df_all = []\n",
    "for df in tqdm(sdf_all):\n",
    "    train_df_all.append(df[0: train_num])\n",
    "    valid_df_all.append(df[train_num: train_num + valid_num])\n",
    "    test_df_all.append(df[train_num + valid_num: train_num + valid_num + test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 2500, 2500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df_all[0]), len(valid_df_all[0]), len(test_df_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90000, 15000, 15000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat(train_df_all, ignore_index=True)\n",
    "valid_df = pd.concat(valid_df_all, ignore_index=True)\n",
    "test_df = pd.concat(test_df_all, ignore_index=True)\n",
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = train_df['sequence']\n",
    "valid_corpus = valid_df['sequence']\n",
    "test_corpus = test_df['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open(\"train.txt\", \"w\", encoding='utf-8')\n",
    "f_valid = open(\"valid.txt\", \"w\", encoding='utf-8')\n",
    "f_test = open(\"test.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "for item in train_corpus:\n",
    "    f_train.write(item + '\\n')\n",
    "f_train.close()\n",
    "\n",
    "for item in valid_corpus:\n",
    "    f_valid.write(item + '\\n')\n",
    "f_valid.close()\n",
    "\n",
    "for item in test_corpus:\n",
    "    f_test.write(item + '\\n')\n",
    "f_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 6000.43it/s]\n"
     ]
    }
   ],
   "source": [
    "tiny_train_num = 1500\n",
    "tiny_valid_num = 250\n",
    "tiny_test_num = 250\n",
    "\n",
    "tiny_train_df_all = []\n",
    "tiny_valid_df_all = []\n",
    "tiny_test_df_all = []\n",
    "for df in tqdm(sdf_all):\n",
    "    tiny_train_df_all.append(df[0: tiny_train_num])\n",
    "    tiny_valid_df_all.append(df[tiny_train_num: tiny_train_num + tiny_valid_num])\n",
    "    tiny_test_df_all.append(df[tiny_train_num + tiny_valid_num: tiny_train_num + tiny_valid_num + tiny_test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 250, 250)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tiny_train_df_all[0]), len(tiny_valid_df_all[0]), len(tiny_test_df_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1500, 1500)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_train_df = pd.concat(tiny_train_df_all, ignore_index=True)\n",
    "tiny_valid_df = pd.concat(tiny_valid_df_all, ignore_index=True)\n",
    "tiny_test_df = pd.concat(tiny_test_df_all, ignore_index=True)\n",
    "len(tiny_train_df), len(tiny_valid_df), len(tiny_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_train_corpus = tiny_train_df['sequence']\n",
    "tiny_valid_corpus = tiny_valid_df['sequence']\n",
    "tiny_test_corpus = tiny_test_df['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = open(\"tiny_train.txt\", \"w\", encoding='utf-8')\n",
    "f_valid = open(\"tiny_valid.txt\", \"w\", encoding='utf-8')\n",
    "f_test = open(\"tiny_test.txt\", \"w\", encoding='utf-8')\n",
    "\n",
    "for item in tiny_train_corpus:\n",
    "    f_train.write(item + '\\n')\n",
    "f_train.close()\n",
    "\n",
    "for item in tiny_valid_corpus:\n",
    "    f_valid.write(item + '\\n')\n",
    "f_valid.close()\n",
    "\n",
    "for item in tiny_test_corpus:\n",
    "    f_test.write(item + '\\n')\n",
    "f_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
